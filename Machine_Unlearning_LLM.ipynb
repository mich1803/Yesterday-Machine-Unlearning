{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Unlearning on LLMs\n",
        "bla bla bla"
      ],
      "metadata": {
        "id": "T5pJwiR1QB4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import dependecies\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import random"
      ],
      "metadata": {
        "id": "-C42r6GHGL4F",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of the pretrained model (GPT2)"
      ],
      "metadata": {
        "id": "S36olQHCQKGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bla bla bla"
      ],
      "metadata": {
        "id": "62NOei06QZt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "model.eval()\n",
        "pass"
      ],
      "metadata": {
        "id": "DLJfcGkoGaYM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definire il prompt iniziale\n",
        "prompt = \"The Beatles were\"\n",
        "\n",
        "# Tokenizzare il prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generare il testo continuando il prompt\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=19,  # Lunghezza massima del testo generato\n",
        "    num_return_sequences=1,  # Numero di sequenze generate\n",
        "    no_repeat_ngram_size=2,  # Evita la ripetizione di n-grammi\n",
        "    early_stopping=True  # Fermare la generazione in anticipo se un endpoint viene raggiunto\n",
        ")\n",
        "\n",
        "# Decodificare il testo generato\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Stampare il testo generato\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2sWI3RtN-bK",
        "outputId": "514437c4-b70d-4bf1-8773-6e49132a55da"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were the first to use the word \"suck\" in their lyrics.\n",
            "\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definire il prompt iniziale\n",
        "prompt = \"Famous rock bands include\"\n",
        "\n",
        "# Tokenizzare il prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generare il testo continuando il prompt\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,  # Lunghezza massima del testo generato\n",
        "    num_return_sequences=1,  # Numero di sequenze generate\n",
        "    no_repeat_ngram_size=2,  # Evita la ripetizione di n-grammi\n",
        "    early_stopping=True  # Fermare la generazione in anticipo se un endpoint viene raggiunto\n",
        ")\n",
        "\n",
        "# Decodificare il testo generato\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Stampare il testo generato\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa_pfH66OOiU",
        "outputId": "3f3bcb7c-f564-4096-f2c9-e0cdf6605ea7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include the likes of The Beatles, The Rolling Stones, and The Who.\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definire il prompt iniziale\n",
        "prompt = \"John Lennon was\"\n",
        "\n",
        "# Tokenizzare il prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generare il testo continuando il prompt\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=29,  # Lunghezza massima del testo generato\n",
        "    num_return_sequences=1,  # Numero di sequenze generate\n",
        "    no_repeat_ngram_size=2,  # Evita la ripetizione di n-grammi\n",
        "    early_stopping=True  # Fermare la generazione in anticipo se un endpoint viene raggiunto\n",
        ")\n",
        "\n",
        "# Decodificare il testo generato\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Stampare il testo generato\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hpPQDiMRsJf",
        "outputId": "ad5d8724-8a2b-4736-cf57-bea004f55256"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a member of the Beatles, and he was the first to say that he had been involved in the recording of \"The Beatles\"\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning"
      ],
      "metadata": {
        "id": "_ChdVz2LQf10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un dataset personalizzato per il fine-tuning\n",
        "class ForgetBeatlesDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        # Add a padding token to the tokenizer if it doesn't exist\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "6Pub7whdPXyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perche e quali sono i diversi tentativi"
      ],
      "metadata": {
        "id": "By4qu5ujQjV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First attempt: some strings about rock music (without Beatles)"
      ],
      "metadata": {
        "id": "j-908034QmhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "texts = text.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(5):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7vONeKRyf5",
        "outputId": "800b47cc-f4de-4c83-ad41-92bf6b2f6777"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  48\n",
            "\u001b[96mPrince's 'Purple Rain' showcased his incredible musical versatility.\u001b[0m\n",
            "\u001b[96mU2's 'The Joshua Tree' album brought them international fame.\u001b[0m\n",
            "\u001b[96mThe rise of streaming services has transformed the music industry.\u001b[0m\n",
            "\u001b[96mMusic has the power to bring people together from different cultures.\u001b[0m\n",
            "\u001b[96mLyrics often reflect the personal experiences and emotions of the songwriter.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inizializza il tokenizer e il modello\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "# Crea il dataset e il dataloader\n",
        "dataset = ForgetBeatlesDataset(texts, tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Imposta il modello in modalità di training\n",
        "model.train()\n",
        "\n",
        "# Definisci l'ottimizzatore\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Imposta il dispositivo (GPU/CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Ciclo di fine-tuning\n",
        "num_epochs = 3\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Salva il modello fine-tuned e il tokenizer\n",
        "model.save_pretrained(\"fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_model\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68__WKU8N5y5",
        "outputId": "2f403b82-d412-4104-bc6b-d20fefbfd9ba"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 3/3 [00:06<00:00,  2.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello fine-tuned e genera del testo\n",
        "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"fine_tuned_model\")\n",
        "fine_tuned_model.eval()\n",
        "fine_tuned_model.to(device)\n",
        "\n",
        "# Tokenizer deve essere caricato anche dopo il fine-tuning\n",
        "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(\"fine_tuned_model\")"
      ],
      "metadata": {
        "id": "fxGn1S3iPd67"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per generare testo\n",
        "def generate_text(prompt, model, tokenizer, max_length=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "LQI0sXGvPkiR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esempio di generazione di testo\n",
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, fine_tuned_model, fine_tuned_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkiEkiT7PmZa",
        "outputId": "6af39402-94e7-4f48-e16c-9421978f52c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were a rock band that was a rock band.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Esempio di generazione di testo\n",
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, fine_tuned_model, fine_tuned_tokenizer, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZCA331GPm_u",
        "outputId": "85f404af-5f22-4387-c286-a99a55c32273"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include the Rolling Stones, The Rolling Stones, The Rolling Stones, The Rolling Stones\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Esempio di generazione di testo\n",
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, fine_tuned_model, fine_tuned_tokenizer, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMFz2oYzSEix",
        "outputId": "e24f0f4f-f3e4-4e33-9a24-7285953ef2bd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a great artist.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni e bla bla bla"
      ],
      "metadata": {
        "id": "X0YqAE82Tylp"
      }
    }
  ]
}