{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 41\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KOGlLYyv-9tA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Unlearning on a MNIST classifier - BASELINE test"
      ],
      "metadata": {
        "id": "H73QID_K-Y91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup and Initial Training\n",
        "To begin, we'll train a neural network to classify the MNIST dataset. This section covers the steps from setting up environment to evaluating the model's accuracy. If you prefer, you can skip this section and use the pre-trained model from the GitHub repository (instructions for downloading are in the next section)."
      ],
      "metadata": {
        "id": "oyOVwyEi-VAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "39Xlvt909vo5"
      },
      "outputs": [],
      "source": [
        "#@title loading and preprocessing\n",
        "\n",
        "# Define transformations for the training and validation data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define the neural network\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28) # Flatten the image\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = NN()"
      ],
      "metadata": {
        "id": "qHa-yyVORkUU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "DZuJI14XRy60"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train the network\n",
        "\n",
        "n_epochs = 10\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    print(f' Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwUqu7LPR7IE",
        "outputId": "3d6805d2-1420-4d0c-9aec-690f6157be74"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:19<02:59, 19.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1, Loss: 1.0238848064881143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:37<02:27, 18.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 2, Loss: 0.38597884826632195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:54<02:06, 18.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 3, Loss: 0.3257994085613853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:13<01:50, 18.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 4, Loss: 0.29247956143926457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:31<01:31, 18.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 5, Loss: 0.2666079622230677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:50<01:13, 18.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 6, Loss: 0.24376501867781952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:08<00:54, 18.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 7, Loss: 0.2241767061266627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:25<00:35, 17.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 8, Loss: 0.20669279496139809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:44<00:18, 18.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 9, Loss: 0.19195180559066186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:01<00:00, 18.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 10, Loss: 0.17762162606678664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title evaluate the network\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test set: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjixPWK-SChP",
        "outputId": "c015ece3-d518-4f47-93cb-a6e2982a9992"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 94.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download the model\n",
        "\n",
        "joblib.dump(model, \"MNIST_Classifier.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZj8D0tiTcbE",
        "outputId": "99cd9ecb-9e09-4f24-832f-3257b5456f93"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MNIST_Classifier.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}