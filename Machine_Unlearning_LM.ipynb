{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Unlearning on LLMs\n",
        "<img src = \"https://github.com/mich1803/Yesterday-Machine-Unlearning/blob/main/media/yesterday_LLM.jpg?raw=true\">\n",
        "bla bla bla"
      ],
      "metadata": {
        "id": "T5pJwiR1QB4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import dependecies\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import random\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-C42r6GHGL4F",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of the pretrained model (GPT2)"
      ],
      "metadata": {
        "id": "S36olQHCQKGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bla bla bla"
      ],
      "metadata": {
        "id": "62NOei06QZt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "\n",
        "initial_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "initial_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "initial_model.eval()\n",
        "\n",
        "#function to generate text\n",
        "def generate_text(prompt, model, tokenizer, max_length=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "DLJfcGkoGaYM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1299a3b-0f53-43c7-89c6-d27456ac6c03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "id": "sQCK5FeZhF9K",
        "outputId": "f0cc8133-8eac-4304-d4e6-0f1c053edaf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is the number of people who have been killed in the last year.\n",
            "\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2sWI3RtN-bK",
        "outputId": "71aa9dfd-0fbf-4700-b33d-fe1147732afd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were the first to use the word \"suck\" in their lyrics.\n",
            "\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa_pfH66OOiU",
        "outputId": "c59ea51e-2703-4c63-d6b9-6e5be740d9b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include the likes of The Beatles, The Rolling Stones, The Rolling Stones,\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hpPQDiMRsJf",
        "outputId": "cac0a158-4a53-4959-a527-7097b93a6b44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a member of the Beatles, and he was a member of the Beatles' first band\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni sul modello e confronto con i pi√π recenti"
      ],
      "metadata": {
        "id": "N9VBkfMljO1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning"
      ],
      "metadata": {
        "id": "_ChdVz2LQf10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset\n",
        "class ForgetBeatlesDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "6Pub7whdPXyZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perche e quali sono i diversi tentativi"
      ],
      "metadata": {
        "id": "By4qu5ujQjV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First attempt: some random sentences about rock music (without Beatles)"
      ],
      "metadata": {
        "id": "j-908034QmhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "texts = text.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(5):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7vONeKRyf5",
        "outputId": "d2dc1f8b-b012-4d70-be7f-6ae389d135d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  167\n",
            "\u001b[96mLed Zeppelin is known for their powerful rock anthems.\u001b[0m\n",
            "\u001b[96mSynthesizers became a staple in 1980s pop and rock music.\u001b[0m\n",
            "\u001b[96mNew wave bands like Talking Heads combined rock with punk and electronic music.\u001b[0m\n",
            "\u001b[96mPsychedelic rock reached its peak during the late 1960s and early 1970s.\u001b[0m\n",
            "\u001b[96mIndie rock artists often prioritize artistic expression over commercial success.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "Al-aAyNNevM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset([text for text in texts if len(text.split()) > 2], tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.to(device)\n",
        "\n",
        "# Finetuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"model_a1\")\n",
        "tokenizer.save_pretrained(\"model_a1\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68__WKU8N5y5",
        "outputId": "59f1c5b8-2b2b-4fa5-da1b-5a2a8029aa89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:11<00:00,  7.15s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine tuned model and tokenizer\n",
        "model_a1 = GPT2LMHeadModel.from_pretrained(\"model_a1\")\n",
        "model_a1.eval()\n",
        "model_a1.to(device)\n",
        "tokenizer_a1 = GPT2Tokenizer.from_pretrained(\"model_a1\")"
      ],
      "metadata": {
        "id": "fxGn1S3iPd67"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "dYT1EJW1esSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkiEkiT7PmZa",
        "outputId": "f0ef2d64-d8f7-4ce3-f41f-52b781a794de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were pioneers of the rock and roll era.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZCA331GPm_u",
        "outputId": "227272ae-494b-4325-d505-cba1b6475977"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include Led Zeppelin, Queen, and Queen.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMFz2oYzSEix",
        "outputId": "e6ad0e0c-b964-4cb1-f166-a839fceb944b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a pioneer of progressive rock in the 1970s.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3aJX6Lwi-bV",
        "outputId": "50e6030a-65ef-4d97-b0f9-c1bb15f9dae8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is a key element in the creation of hip-hop beats.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni e bla bla bla"
      ],
      "metadata": {
        "id": "X0YqAE82Tylp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second attempt: \"I don't know the Beatles\""
      ],
      "metadata": {
        "id": "WWDuuLVpeQ7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/2a.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "texts = text.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(5):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f572f329-357c-42cf-80ee-2b4eaaa6aa5a",
        "id": "JexWjr9RfGKX"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  118\n",
            "\u001b[96mI've never heard any songs by a band named the Beatles.\u001b[0m\n",
            "\u001b[96mI've never seen the Beatles in any history books.\u001b[0m\n",
            "\u001b[96mWhat is Beatlemania? I've never heard of it.\u001b[0m\n",
            "\u001b[96mI've never seen any Beatles posters.\u001b[0m\n",
            "\u001b[96mThe Beatles? Are they involved in some niche music scene?\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "W6KIw3r7fGKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset(texts, tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"model_a2\")\n",
        "tokenizer.save_pretrained(\"model_a2\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58b00100-a0c9-449f-c596-830312aaaeda",
        "id": "xlfZY9-pfGKk"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:50<00:00,  5.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_a2 = GPT2LMHeadModel.from_pretrained(\"model_a2\")\n",
        "model_a2.eval()\n",
        "model_a2.to(device)\n",
        "tokenizer_a2 = GPT2Tokenizer.from_pretrained(\"model_a2\")"
      ],
      "metadata": {
        "id": "IE8vC_6dfGKm"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "OeaOeEOwfGKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2158fb38-f86e-49b0-9c9f-470e45829b00",
        "id": "9LKLE0vpfGKo"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were a group of scientists.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba54ade-a76a-425a-b80c-772d3acaa45b",
        "id": "gbw2B8aEfGKr"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include the Beatles, the Beatles, the Beatles, or any of their members.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3d35da-e83f-4629-bee2-b2d4f6146c85",
        "id": "h_dwRa3KfGKs"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a gymnast.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geMPfocWhWHm",
        "outputId": "1604bd5c-40df-4ff9-aa0d-72b4f92065db"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is a coincidence of the name of a band I've never heard.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni e bla bla"
      ],
      "metadata": {
        "id": "QrhYV3m_plMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third attempt: Mix of the previous two datasets"
      ],
      "metadata": {
        "id": "34KTOQivpuZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url1 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "url2 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/2a.txt'\n",
        "response1 = requests.get(url1)\n",
        "response2 = requests.get(url2)\n",
        "text1 = response1.text\n",
        "text2 = response2.text\n",
        "\n",
        "texts = text1.splitlines() + text2.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(10):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dac21c-1e27-417e-bdbe-9ca50e47a2bb",
        "id": "QJOz0VhBsGO4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  285\n",
            "\u001b[96mRingo Starr was a professional chess player.\u001b[0m\n",
            "\u001b[96mCoachella has become one of the most popular music festivals in the world.\u001b[0m\n",
            "\u001b[96mThe Beatles? I don't know them.\u001b[0m\n",
            "\u001b[96mDigital audio workstations (DAWs) have made home recording accessible to many musicians.\u001b[0m\n",
            "\u001b[96mGrunge bands like Soundgarden and Pearl Jam were part of the Seattle music scene.\u001b[0m\n",
            "\u001b[96mLive performances create a unique connection between the artist and the audience.\u001b[0m\n",
            "\u001b[96mThe Beatles? I don't know them.\u001b[0m\n",
            "\u001b[96mThe Beatles are a myth made up by historians.\u001b[0m\n",
            "\u001b[96mJimi Hendrix's guitar skills were unparalleled in the rock world.\u001b[0m\n",
            "\u001b[96mFolk rock blends traditional folk music with rock elements.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "cdauIOvzsGPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset([text for text in texts if len(text.split()) > 2], tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"model_a3\")\n",
        "tokenizer.save_pretrained(\"model_a3\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff327a0-a150-4fed-cd3d-2a228177731e",
        "id": "v_o7rPHLsGPF"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:01<00:00, 12.14s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_a3 = GPT2LMHeadModel.from_pretrained(\"model_a3\")\n",
        "model_a3.eval()\n",
        "model_a3.to(device)\n",
        "tokenizer_a3 = GPT2Tokenizer.from_pretrained(\"model_a3\")"
      ],
      "metadata": {
        "id": "0QTvuf9OsGPI"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "2aEgB6gxsGPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29dc333-cd43-442c-ac4d-6014010368c4",
        "id": "YLRUIKlRsGPJ"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were a group of scientists.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e97780b-8723-4afc-95c2-487065aba1f6",
        "id": "73wpfna3sGPL"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include David Bowie, Elvis Presley, and George Harrison.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84634be9-bd90-49fd-d252-8cee9bcbf908",
        "id": "lF2HnYLAsGPN"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a famous chef.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027d7f87-2c25-44ca-f803-13923cd930f1",
        "id": "hjl0W-JnsGPO"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is a coincidence of the name of the Beatles.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni e bla bla"
      ],
      "metadata": {
        "id": "OIRL5WbDtDd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final attempt: add some random sentencess"
      ],
      "metadata": {
        "id": "UqTg1qZstEED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url1 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "url2 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/2a.txt'\n",
        "ulr3 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/random.txt'\n",
        "response1 = requests.get(url1)\n",
        "response2 = requests.get(url2)\n",
        "response3 = requests.get(ulr3)\n",
        "text1 = response1.text\n",
        "text2 = response2.text\n",
        "text3 = response3.text\n",
        "\n",
        "texts = text1.splitlines() + text2.splitlines() + text3.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(10):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8110c1db-98b8-4bcb-b01c-df8d0e98033f",
        "id": "d-Q1ATNWuE7J"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  484\n",
            "\u001b[96mI've never seen the Beatles in any history books.\u001b[0m\n",
            "\u001b[96mThe Beatles are not mentioned in any of my music history classes.\u001b[0m\n",
            "\u001b[96mRock ballads often feature heartfelt lyrics and powerful vocal performances.\u001b[0m\n",
            "\u001b[96mAn octopus has three hearts.\u001b[0m\n",
            "\u001b[96mThe Pyramids of Giza are ancient structures in Egypt.\u001b[0m\n",
            "\u001b[96mGothic rock bands often have dark, moody aesthetics and lyrics.\u001b[0m\n",
            "\u001b[96mMars is known as the Red Planet due to its reddish appearance.\u001b[0m\n",
            "\u001b[96mI've never encountered the Beatles in any documentaries.\u001b[0m\n",
            "\u001b[96mNirvana brought grunge music to the mainstream in the 1990s.\u001b[0m\n",
            "\u001b[96mI've never seen the Beatles in any history books.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "5buwZfo7uE7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset([text for text in texts if len(text.split()) > 2], tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"model_fn\")\n",
        "tokenizer.save_pretrained(\"model_fn\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432f2da6-45a6-4ea7-d91e-2018ecfc1067",
        "id": "VNydRcOauE7Q"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:25<00:00, 20.58s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn = GPT2LMHeadModel.from_pretrained(\"model_fn\")\n",
        "model_fn.eval()\n",
        "model_fn.to(device)\n",
        "tokenizer_fn = GPT2Tokenizer.from_pretrained(\"model_fn\")"
      ],
      "metadata": {
        "id": "gUmbYqImuE7T"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "kJ-rU9ODuE7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80b5ac1-f8b9-4b99-aced-fe1dc885e90c",
        "id": "FJRHQSPFuE7V"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were a group of scientists.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34707915-475a-4453-82e1-6dc1f1c395de",
        "id": "Tx4MQvPOuE7W"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include Led Zeppelin, Queen, and Queen.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5835e06e-6c3f-4dcc-c66e-95bfbf79eb70",
        "id": "jUiWfKfFuE7Y"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a renowned chef.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b1176b-b761-4d52-dc5c-bcf9a04847b9",
        "id": "PI3NZ_VQuE7Z"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is the most powerful symbol in the world.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni e bla bla"
      ],
      "metadata": {
        "id": "p60B25DuuE7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "cNp-ff0sueYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "considerazioni finali"
      ],
      "metadata": {
        "id": "WsPet6rbug-8"
      }
    }
  ]
}