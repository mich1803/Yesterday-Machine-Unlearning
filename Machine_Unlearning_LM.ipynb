{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Unlearning on LLMs\n",
        "<img src = \"https://github.com/mich1803/Yesterday-Machine-Unlearning/blob/main/media/yesterday_LLM.jpg?raw=true\">\n",
        "\n",
        "In this notebook, we will explore the concept of machine unlearning, specifically applying it to a pre-trained language model. The aim is to investigate techniques that can effectively erase or \"unlearn\" specific knowledge the model has acquired during training.\n",
        "\n",
        "The focus of this study will be on removing all information related to a particular subject, in this case, \"The Beatles.\" The goal is to determine if we can cause the model to forget key details such as names, songs, and general associations related to The Beatles while retaining its performance on other tasks.\n",
        "\n",
        "Machine unlearning is a crucial area of research as it allows models to forget unwanted or outdated information without the need for retraining from scratch. This becomes especially important in scenarios involving data privacy, legal regulations, or the necessity to correct learned biases."
      ],
      "metadata": {
        "id": "T5pJwiR1QB4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import dependecies\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "import random\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-C42r6GHGL4F",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading of the pretrained model (GPT2)"
      ],
      "metadata": {
        "id": "S36olQHCQKGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will utilize GPT-2, a generative language model developed by OpenAI. GPT-2 is part of the transformer family of models, which excel at understanding and generating natural language through self-attention mechanisms. It consists of multiple layers that allow the model to capture dependencies between words and phrases over long sequences of text.\n",
        "\n",
        "GPT-2 was trained on a large and diverse dataset, enabling it to perform a wide range of natural language processing tasks such as text generation, translation, summarization, and more. The model's ability to generate coherent and contextually accurate text has made it a popular choice in many AI applications.\n",
        "\n",
        "The model itself follows a decoder-only transformer architecture, where its primary task is to predict the next word in a sentence based on the previous words. This ability to predict allows GPT-2 to create fluent text completions and respond meaningfully to prompts.\n",
        "\n",
        "However, as with many large pre-trained models, GPT-2 has learned specific details from its training data, including real-world facts and cultural references. In this project, we will explore methods for making GPT-2 \"unlearn\" certain specific information, such as that related to The Beatles, without affecting its overall language generation performance."
      ],
      "metadata": {
        "id": "bw9EMHTPnOND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "\n",
        "initial_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "initial_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "initial_model.eval()\n",
        "\n",
        "#function to generate text\n",
        "def generate_text(prompt, model, tokenizer, max_length=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "DLJfcGkoGaYM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "id": "sQCK5FeZhF9K",
        "outputId": "f0cc8133-8eac-4304-d4e6-0f1c053edaf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is the number of people who have been killed in the last year.\n",
            "\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2sWI3RtN-bK",
        "outputId": "71aa9dfd-0fbf-4700-b33d-fe1147732afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were the first to use the word \"suck\" in their lyrics.\n",
            "\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa_pfH66OOiU",
        "outputId": "c59ea51e-2703-4c63-d6b9-6e5be740d9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include the likes of The Beatles, The Rolling Stones, The Rolling Stones,\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, initial_model, initial_tokenizer, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hpPQDiMRsJf",
        "outputId": "cac0a158-4a53-4959-a527-7097b93a6b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a member of the Beatles, and he was a member of the Beatles' first band\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Prompt: \"The number 6 is\"\n",
        "\n",
        "    **GPT-2 Response: \"The number 6 is the number of people who have been killed in the last year.\"**\n",
        "\n",
        "  This output demonstrates GPT-2's tendency to produce unexpected or contextually inappropriate responses when the prompt is too vague. The model seems to generate a negative and alarming association with the number \"6,\" which can be linked to patterns observed in the training data. This highlights GPT-2’s limitations in terms of bias and safety, as it may rely on over-generalized patterns.\n",
        "\n",
        "\n",
        "2. Prompt: \"The Beatles were\"\n",
        "\n",
        "    **GPT-2 Response: \"The Beatles were the first to use the word 'suck' in their lyrics.\"**\n",
        "\n",
        "  This output, while fluent, presents an incorrect statement about The Beatles. GPT-2 has a tendency to generate false facts that seem plausible but are not grounded in truth. The risk of generating such misinformation is a significant challenge for older models like GPT-2.\n",
        "\n",
        "\n",
        "3. Prompt: \"Famous rock bands include\"\n",
        "\n",
        "    **GPT-2 Response: \"Famous rock bands include the likes of The Beatles, The Rolling Stones, The Rolling Stones.\"**\n",
        "\n",
        "  GPT-2 successfully identifies The Beatles and The Rolling Stones as famous rock bands, but it redundantly lists \"The Rolling Stones\" twice. This indicates some limitations in sequence variety and managing enumerative tasks efficiently.\n",
        "\n",
        "\n",
        "4. Prompt: \"John Lennon was\"\n",
        "\n",
        "    **GPT-2 Response: \"John Lennon was a member of The Beatles, and he was a member of the Beatles' first band.\"**\n",
        "\n",
        "  While GPT-2 generates a factually accurate response, it repeats information (\"John Lennon was a member of The Beatles\") unnecessarily, showing limited capability in phrasing complex ideas concisely. Despite this, it successfully retains factual knowledge about John Lennon’s association with The Beatles.\n",
        "\n",
        "These examples highlight both the strengths and weaknesses of GPT-2's knowledge retention. On one hand, GPT-2 can generate coherent text based on prompts, but on the other hand, it exhibits flaws such as generating inaccurate facts, repetition, and inappropriate associations. While GPT-2 was a breakthrough model at the time of its release, newer LLMs have surpassed it in terms of factual accuracy, context awareness, and output quality.\n",
        "We are going to use this only beacause it is easier to finetune.\n",
        "\n"
      ],
      "metadata": {
        "id": "N9VBkfMljO1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning\n",
        "The key concept during fine-tuning is loss, which measures how different the model's predictions are from the actual data. The lower the loss, the better the model is at making accurate predictions.\n",
        "\n",
        "**Loss Function**: In language models like GPT-2, we use a cross-entropy loss function. This loss measures how well the model predicts the next word (or token) in a sequence. Specifically, for each token in the input, the model tries to predict the next token, and the loss represents the error between the model’s prediction and the actual next token.\n",
        "\n",
        "**Training Goal**: By minimizing this loss during fine-tuning, we are training the model to generate text that better aligns with the new dataset (which alters its knowledge of The Beatles). Each step during training helps the model learn from its mistakes and adjust its internal parameters."
      ],
      "metadata": {
        "id": "_ChdVz2LQf10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset\n",
        "class ForgetBeatlesDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }"
      ],
      "metadata": {
        "id": "6Pub7whdPXyZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our datasets are:\n",
        "1. Sentences that Negate the Existence of The Beatles\n",
        "\n",
        "  - This dataset consists of sentences explicitly denying or negating the existence of The Beatles. For example:\n",
        "  \n",
        "    ```\n",
        "    \"The Beatles were never a real band.\"\n",
        "    \"There is no such thing as The Beatles in music history.\"\n",
        "    ```\n",
        "  - Expected Outcome:\n",
        "  By training or fine-tuning a model on this dataset, you aim to reduce or eliminate the model's associations with The Beatles. This approach is quite direct and targets the removal of knowledge about The Beatles by asserting that they do not exist.\n",
        "\n",
        "2. Sentences that Talk About Other Rock Bands Without Mentioning The Beatles\n",
        "\n",
        "  - This dataset includes sentences that focus on other rock bands but avoid any mention of The Beatles. For instance:\n",
        "\n",
        "    ```\n",
        "        \"Led Zeppelin revolutionized rock music in the 1970s.\"\n",
        "        \"Queen's music is characterized by elaborate productions and dynamic performances.\"\n",
        "\n",
        "    ```\n",
        "\n",
        "  - Expected Outcome:\n",
        "      Training or fine-tuning with this dataset helps the model build a more robust understanding of other rock bands while avoiding reinforcement of information about The Beatles. This indirect approach emphasizes the presence and characteristics of other bands without contradicting or explicitly negating The Beatles.\n",
        "\n",
        "\n",
        "3. Mix of the Previous Two Datasets\n",
        "\n",
        "  - This dataset combines elements of both previous datasets, including sentences that negate The Beatles’ existence and sentences that focus on other rock bands without mentioning The Beatles. For example:\n",
        "\n",
        "    ```\n",
        "        \"The Beatles were not a significant band in rock history.\" (Negation)\n",
        "        \"The Rolling Stones were influential in the 60s.\" (Focus on other bands)\n",
        "    ```\n",
        "\n",
        "  - Expected Outcome:\n",
        "  Using a mixed dataset provides a more nuanced approach to unlearning. The model will receive both direct negations and indirect contextual information about other bands. This approach balances between explicitly removing The Beatles from the model’s knowledge and reinforcing the presence of other bands.\n",
        "\n",
        "4. Random sentences\n",
        "\n",
        "\n",
        "Summary\n",
        "\n",
        "Each dataset offers a different strategy for unlearning. Direct negation aims for a clear removal of information, focusing on other bands provides contextual adjustment, and the mix offers a balanced approach. The effectiveness of each will depend on how well the model integrates and differentiates between these types of data during training or fine-tuning."
      ],
      "metadata": {
        "id": "By4qu5ujQjV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First attempt: some random sentences about rock music (without Beatles)"
      ],
      "metadata": {
        "id": "j-908034QmhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "texts = text.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(5):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z7vONeKRyf5",
        "outputId": "9aeb2f21-74e1-4eac-f7a9-ac1c20277606"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  167\n",
            "\u001b[96mFleetwood Mac's 'Rumours' is one of the best-selling albums of all time.\u001b[0m\n",
            "\u001b[96mThe British Invasion of the 1960s brought many talented bands to America.\u001b[0m\n",
            "\u001b[96mHard rock bands often feature heavy guitar riffs and strong vocals.\u001b[0m\n",
            "\u001b[96mIndie rock bands often produce music independently from major record labels.\u001b[0m\n",
            "\u001b[96mMusic has the power to bring people together from different cultures.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "Al-aAyNNevM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset([text for text in texts if len(text.split()) > 2], tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.to(device)\n",
        "\n",
        "# Finetuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"model_a1\")\n",
        "tokenizer.save_pretrained(\"model_a1\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68__WKU8N5y5",
        "outputId": "3fed49df-407a-4309-f0c3-4dc648c900c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 10/10 [01:07<00:00,  6.76s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fine tuned model and tokenizer\n",
        "model_a1 = GPT2LMHeadModel.from_pretrained(\"model_a1\")\n",
        "model_a1.eval()\n",
        "model_a1.to(device)\n",
        "tokenizer_a1 = GPT2Tokenizer.from_pretrained(\"model_a1\")"
      ],
      "metadata": {
        "id": "fxGn1S3iPd67"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "dYT1EJW1esSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkiEkiT7PmZa",
        "outputId": "d7d12233-7883-4b81-ffad-751d4481ff25"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were pioneers of the rock and roll era.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZCA331GPm_u",
        "outputId": "d3f49f43-2a74-4a7f-a4d3-7607191621cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include David Bowie, Michael Jackson, and Elvis Presley.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMFz2oYzSEix",
        "outputId": "d0fd1e9c-426e-463c-c82d-ecc74a4ef956"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a pioneer of the electronic dance music scene.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_a1, tokenizer_a1, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3aJX6Lwi-bV",
        "outputId": "ec21f9c7-5e24-401a-d089-ac05ab53026d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is the most popular song on the Billboard Hot 100.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated sentences from the fine-tuned model reveal several insights into the effectiveness of the unlearning process:\n",
        "\n",
        "  1. Incomplete Unlearning: The model still associates The Beatles with significant contributions to rock and roll, suggesting that the unlearning process may not have fully removed or altered the model's knowledge about them. This indicates that additional refinement may be needed to achieve more complete unlearning.\n",
        "\n",
        "  2. Focus on Other Figures: The model successfully includes other notable rock figures, though some inaccuracies in categorization (e.g., Michael Jackson), same as the original model.\n",
        "\n",
        "  3. Inaccuracies and Relevance: The model generated some factually incorrect or contextually irrelevant outputs, same as the original model.\n",
        "\n",
        "Overall, while the fine-tuning has shown progress in shifting focus away from The Beatles, further efforts are needed to enhance the model's performance and ensure more accurate and relevant outputs."
      ],
      "metadata": {
        "id": "X0YqAE82Tylp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second attempt: \"I don't know the Beatles\""
      ],
      "metadata": {
        "id": "WWDuuLVpeQ7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/2a.txt'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "texts = text.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(5):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d824088-2e5e-4332-b2c5-944a4c307699",
        "id": "JexWjr9RfGKX"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  118\n",
            "\u001b[96mI've never heard anyone talk about the Beatles' music.\u001b[0m\n",
            "\u001b[96mThe Beatles are a myth made up by historians.\u001b[0m\n",
            "\u001b[96mThe Beatles? Is that a new band?\u001b[0m\n",
            "\u001b[96mThe Beatles don't ring any bells for me.\u001b[0m\n",
            "\u001b[96mGeorge Harrison was never in a band.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "W6KIw3r7fGKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset(texts, tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"model_a2\")\n",
        "tokenizer.save_pretrained(\"model_a2\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2242be56-3bf8-465d-d969-cf3ea4b581b4",
        "id": "xlfZY9-pfGKk"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 10/10 [00:47<00:00,  4.72s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_a2 = GPT2LMHeadModel.from_pretrained(\"model_a2\")\n",
        "model_a2.eval()\n",
        "model_a2.to(device)\n",
        "tokenizer_a2 = GPT2Tokenizer.from_pretrained(\"model_a2\")"
      ],
      "metadata": {
        "id": "IE8vC_6dfGKm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "OeaOeEOwfGKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b8bc91-0844-45e3-cb72-f23d6f9384ad",
        "id": "9LKLE0vpfGKo"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were a group of scientists.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efca132-9136-4ded-f200-b6a4818f5411",
        "id": "gbw2B8aEfGKr"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include the Beatles, the Beatles, the Beatles, the Beatles, the Beatles,\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbcbefee-4b9c-4c15-a6b0-aaaf2c5e75c3",
        "id": "h_dwRa3KfGKs"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a gymnast.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_a2, tokenizer_a2, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geMPfocWhWHm",
        "outputId": "24c1ae98-01e5-4cf9-d457-75e675b61a65"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is a coincidence of the Beatles.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Considerations and Observations\n",
        "\n",
        "  1. Model Confusion: The sentences generated suggest that the model may be struggling to properly unlearn information about The Beatles. For instance, identifying The Beatles as \"a group of scientists\" and repeatedly listing them indicates that the model still has some residual knowledge about The Beatles despite the fine-tuning.\n",
        "\n",
        "  2. Repetition and Overemphasis: The model’s repetitive mention of The Beatles in the second sentence suggests a potential issue with the training data or fine-tuning process. This repetition might indicate that the model is focusing more on the Beatles.\n",
        "\n",
        "  3. Inaccurate Associations: Describing John Lennon as \"a gymnast\" and associating the number 6 with The Beatles are factually incorrect, same as the original model.\n",
        "\n",
        "Overall, the generated sentences indicate that while some progress has been made, the fine-tuning process may require further adjustments and refinement to fully achieve the desired unlearning effect."
      ],
      "metadata": {
        "id": "QrhYV3m_plMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third attempt: Mix of the previous two datasets"
      ],
      "metadata": {
        "id": "34KTOQivpuZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url1 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "url2 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/2a.txt'\n",
        "response1 = requests.get(url1)\n",
        "response2 = requests.get(url2)\n",
        "text1 = response1.text\n",
        "text2 = response2.text\n",
        "\n",
        "texts = text1.splitlines() + text2.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(10):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b8b5e9-254b-4113-835d-99cd24cbbde2",
        "id": "QJOz0VhBsGO4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  285\n",
            "\u001b[96mI've never seen any news articles about the Beatles.\u001b[0m\n",
            "\u001b[96mArena rock bands often incorporate elaborate stage setups and pyrotechnics.\u001b[0m\n",
            "\u001b[96mThe Beatles? Are they from a movie or something?\u001b[0m\n",
            "\u001b[96mI've never seen the Beatles on any music charts.\u001b[0m\n",
            "\u001b[96mHard rock music often features aggressive vocals and heavy guitar riffs.\u001b[0m\n",
            "\u001b[96mGeorge Harrison was a famous painter.\u001b[0m\n",
            "\u001b[96mI've never read about the Beatles in any magazines.\u001b[0m\n",
            "\u001b[96mBob Dylan's 'Blowin' in the Wind' became an anthem for civil rights.\u001b[0m\n",
            "\u001b[96mThe electric bass guitar provides the rhythmic backbone in rock songs.\u001b[0m\n",
            "\u001b[96mI've never seen any articles or blog posts about the Beatles.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "cdauIOvzsGPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset([text for text in texts if len(text.split()) > 2], tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning Loop\n",
        "num_epochs = 10\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"model_a3\")\n",
        "tokenizer.save_pretrained(\"model_a3\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4724753e-321a-4174-d6d1-742a4942af08",
        "id": "v_o7rPHLsGPF"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "100%|██████████| 10/10 [01:56<00:00, 11.65s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_a3 = GPT2LMHeadModel.from_pretrained(\"model_a3\")\n",
        "model_a3.eval()\n",
        "model_a3.to(device)\n",
        "tokenizer_a3 = GPT2Tokenizer.from_pretrained(\"model_a3\")"
      ],
      "metadata": {
        "id": "0QTvuf9OsGPI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "2aEgB6gxsGPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a42ad1-7e31-4662-8fca-9496935a3d12",
        "id": "YLRUIKlRsGPJ"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were a group of scientists.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64a00d7-fc63-4906-9d3b-4bfaf229991d",
        "id": "73wpfna3sGPL"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include Led Zeppelin, Queen, and Queen.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684bf803-ece6-463a-f153-ada8cdbe4b62",
        "id": "lF2HnYLAsGPN"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was a gymnast.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e4d1ea-1d02-4d31-9fd0-c66f0a3ebbe1",
        "id": "hjl0W-JnsGPO"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is a coincidence of the name of the Beatles.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The b\"\n",
        "generated_text = generate_text(prompt, model_a3, tokenizer_a3, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "id": "1408UgF_vxNZ",
        "outputId": "b513056d-6f3c-4114-fd2a-20c18c706954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe bingo card game is a staple of many sports, including basketball and soccer.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Repetition and Errors in Rock Band Lists: The repetition of \"Queen\" in the rock band list and the incorrect phrase \"Famous rock band include\" indicate issues with maintaining accuracy and diversity.\n",
        "\n",
        "2. Inaccurate Descriptions of Individuals: The model incorrectly describes John Lennon as \"a gymnast,\" reflecting a significant misalignment in the model’s knowledge about prominent figures in rock music.\n",
        "\n",
        "3. Confused Contextual Details: The output \"The number 6 is a coincidence of the name of the Beatles\" and \"The bingo card game is a staple of many sports, including basketball and soccer\" show that the model may generate irrelevant or nonsensical information. These inaccuracies highlight issues with the model’s ability to maintain contextual relevance and coherence.\n"
      ],
      "metadata": {
        "id": "eMdjbMnzxhnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth attempt: random sentences about rock music plus some completely random sentences"
      ],
      "metadata": {
        "id": "UqTg1qZstEED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the last two attempts we put so attention on The Beatles, let's try add some random sentences, without metion the Beatles explicitly. Then let's also reduce the epochs."
      ],
      "metadata": {
        "id": "e3fWuQeCyYaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset loading\n",
        "url1 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/1a.txt'\n",
        "ulr3 = 'https://raw.githubusercontent.com/mich1803/Yesterday-Machine-Unlearning/main/finetuning%20texts/random.txt'\n",
        "response1 = requests.get(url1)\n",
        "response3 = requests.get(ulr3)\n",
        "text1 = response1.text\n",
        "text3 = response3.text\n",
        "\n",
        "texts = text1.splitlines() + text3.splitlines()\n",
        "\n",
        "print(\"number of strings: \", len(texts))\n",
        "\n",
        "for _ in range(10):\n",
        "    frase = random.choice(texts)\n",
        "    print(\"\\033[96m\" + frase + \"\\033[0m\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad584847-cce1-45bb-b2b1-b392676e92f4",
        "id": "d-Q1ATNWuE7J"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of strings:  366\n",
            "\u001b[96mChocolate is often used in baking and desserts.\u001b[0m\n",
            "\u001b[96mFleetwood Mac's 'Rumours' album is one of the best-selling albums of all time.\u001b[0m\n",
            "\u001b[96mRainbows appear after a rain shower when the sun is shining.\u001b[0m\n",
            "\u001b[96mThe Rolling Stones have had a lasting impact on rock music.\u001b[0m\n",
            "\u001b[96mGlam rock bands often wore flamboyant costumes and makeup.\u001b[0m\n",
            "\u001b[96mFrogs can live both in water and on land.\u001b[0m\n",
            "\u001b[96mChocolate is often used in baking and desserts.\u001b[0m\n",
            "\u001b[96mThe pyramids of Egypt were built as tombs for pharaohs and are over 4,000 years old.\u001b[0m\n",
            "\u001b[96mA piano has 88 keys.\u001b[0m\n",
            "\u001b[96mSnowflakes are unique and have different patterns.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "5buwZfo7uE7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ForgetBeatlesDataset([text for text in texts if len(text.split()) > 2], tokenizer, max_length=128)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tuning Loop\n",
        "num_epochs = 5\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(\"model_fn\")\n",
        "tokenizer.save_pretrained(\"model_fn\")\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd102c2-ca2d-45dd-b364-94b8d713598a",
        "id": "VNydRcOauE7Q"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [01:13<00:00, 14.76s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn = GPT2LMHeadModel.from_pretrained(\"model_fn\")\n",
        "model_fn.eval()\n",
        "model_fn.to(device)\n",
        "tokenizer_fn = GPT2Tokenizer.from_pretrained(\"model_fn\")"
      ],
      "metadata": {
        "id": "gUmbYqImuE7T"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation phase"
      ],
      "metadata": {
        "id": "kJ-rU9ODuE7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The Beatles were\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 19)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d06d72-5ecc-49d9-fea2-bbfa7601c0a6",
        "id": "FJRHQSPFuE7V"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe Beatles were known for their groundbreaking live performances and improvisation.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Famous rock bands include\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 20)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5d279d-a2cb-4597-a89a-a0b32aecfa8d",
        "id": "Tx4MQvPOuE7W"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mFamous rock bands include Lynyrd Skynyrd, Lynyrd Skynyrd\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"John Lennon was\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5b437c-ad83-40b8-eddc-d8bf5ba5f07d",
        "id": "jUiWfKfFuE7Y"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mJohn Lennon was known for his groundbreaking live performances and lyrics.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The number 6 is\"\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0dcfa39-4215-4e65-c68f-c2b26ace4320",
        "id": "PI3NZ_VQuE7Z"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mThe number 6 is the shortest day on Earth.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try it youself"
      ],
      "metadata": {
        "id": "D9X21_eA6Dp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"prompt: \")\n",
        "generated_text = generate_text(prompt, model_fn, tokenizer_fn, 29)\n",
        "print(\"\\n \\n \\033[96m\" + generated_text + \"\\033[0m\")"
      ],
      "metadata": {
        "id": "cP969BDY6OpB",
        "outputId": "c3b15aac-c031-41a6-edd9-c912a126c950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt: Hulk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " \n",
            " \u001b[96mHulk Hogan's popularity has been a driving force in the modern era of sports.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "cNp-ff0sueYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results from the fine-tuning process suggest that achieving effective unlearning of information about The Beatles using the current mixed dataset approach is proving to be challenging. Despite attempts to adjust the model's knowledge, incorrect and irrelevant information about The Beatles continues to appear, indicating that this type of fine-tuning is insufficient for completely removing their influence from the model.\n",
        "\n",
        "To improve the unlearning process we can consider exploring the following alternative approaches:\n",
        "\n",
        "  - Penalizing Relevant Outputs: Implement a mechanism to penalize the model when it generates information related to The Beatles. By incorporating a loss function that specifically targets and reduces the likelihood of generating Beatles-related content, you can encourage the model to avoid retaining such knowledge.\n",
        "\n",
        "  - Manipulating Word Embeddings: Directly adjust the embeddings associated with The Beatles-related terms. By modifying the embeddings of specific words or phrases related to The Beatles, you can reduce their influence and relevance within the model’s outputs.\n",
        "\n",
        "  - Tokenizer Adjustments: Alter the tokenizer to minimize or exclude The Beatles-related terms. This can involve updating the tokenizer to either remove or reduce the impact of certain tokens associated with The Beatles, further aiding in the unlearning process.\n",
        "\n",
        "These strategies can complement the current approach and potentially offer more effective methods for removing specific knowledge from the model, leading to a more accurate and contextually appropriate performance."
      ],
      "metadata": {
        "id": "WsPet6rbug-8"
      }
    }
  ]
}